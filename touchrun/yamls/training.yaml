# Service configuration for multinode.
apiVersion: v1
kind: Service
metadata:
  name: multinode
  labels:
    app: multinode
spec:
  clusterIP: None  # ClusterIP set to None for headless service.
  ports:
  - name: nccl  # Port for torchrun master-worker node communication.
    port: 29500
    targetPort: 29500
  selector:
    job-name: multinode  # Selector for pods associated with this service.

---

# Job configuration for multinode training.
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: multinode  # Label for identifying the pods to the Service.
  name: multinode
spec:
  completionMode: Indexed
  completions: 2  # Should match the number of nodes.
  parallelism: 2  # Should match the number of nodes.
  template:
    metadata:
      annotations:
        linkerd.io/inject: disabled  # Disable service mesh as it might interfere with networking.
    spec:
      containers:
      - image: your-image-name  # Image built from Dockerfile.
        name:
          multinode
        args:
        - sh  # Run command with shell to evaluate environment variables at runtime
        - -c
        - torchrun --nproc_per_node $NGPUS --nnodes $NNODES --node_rank $JOB_COMPLETION_INDEX --master_addr $MASTER_ADDR --master_port $MASTER_PORT train.py
        env:
        - name: MASTER_ADDR
          value: multinode-0.multinode  # Node with rank 0 is chosen as the master node.
        - name: MASTER_PORT
          value: '29500'
        - name: NNODES
          value: '2'  # Number of training nodes.
        - name: NGPUS
          value: '4'  # Number of GPUs in the machine.
        ports:
        - containerPort: 29500
          name: nccl
        resources:
          limits:
            cpu: 46000m
            memory: 160Gi
            nvidia.com/gpu: '4'  # Request the max available amount of GPUs in your machine.
          requests:
            cpu: 46000m
            memory: 160Gi
        volumeMounts:
        - mountPath: /dev/shm  # Mounting emptyDir as shared memory for communication within nodes.
          name: shm
      nodeSelector:  # Replace with desired method of node scheduling.
        infra.kensho.com/instancegroup: gpu-g4-12-nodes
      restartPolicy: Never
      subdomain: multinode  # Required for pod-to-pod communication in Indexed-Jobs.
      volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 13Gi
        name: shm
